[basic]
train_enc = 'data/train.enc'
train_dec = 'data/train.dec'
test_enc = 'data/test.enc'
test_dec = 'data/test.enc'
# vocabulary size 
# 	20,000 is a reasonable size
enc_vocab_size = 20000
dec_vocab_size = 20000
# folder where checkpoints, vocabulary, temporary data will be stored
working_directory = 'data/'
[advanced]
learning_rate = 0.5
learning_rate_decay_factor = 0.99
max_gradient_norm = 5.0
batch_size = 64
# typical options : 128, 256, 512, 1024
layer_size = 128
# dataset size limit; typically none : no limit
max_train_data_size = 0
# steps per checkpoint
# 	Note : At a checkpoint, models parameters are saved, model is evaluated
#			and results are printed
steps_per_checkpoint = 200
##############################################################################
# Note : Edit the bucket sizes at line74 (Edit this) of execute.py
# 
#	Learn more about the configurations from this link
#		https://www.tensorflow.org/versions/r0.9/tutorials/seq2seq/index.html
##############################################################################
